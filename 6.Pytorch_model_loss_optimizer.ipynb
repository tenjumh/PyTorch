{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) Define Dataset\n",
    "- 2) Define Model\n",
    "- 3) Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 4, #features: 1\n",
      "Prediction before training: f(5) = -1.245\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_sample, n_feature = X.shape\n",
    "#print(n_sample, n_feature)\n",
    "print(f'#samples: {n_sample}, #features: {n_feature}')\n",
    "\n",
    "input_size = n_feature\n",
    "output_size = n_feature\n",
    "\n",
    "#model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# defice loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 1000\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) epoch 정의\n",
    "- 2) 정의된 model에 일단 X를 넣어서 예측하는 변수 지정(y_predicted)\n",
    "- 3) 정의된 loss에 Y와 y_predicted 넣어서 변수 지정(I)\n",
    "- 4) forward만 했으니 loss value를 대상으로 backward()\n",
    "- 5) 그리고 optimizer.step()\n",
    "- 6) optimizer.zero_grad() 초기화???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1 : w =  1.9991527795791626  loss =  tensor(1.0425e-06, grad_fn=<MseLossBackward>)\n",
      "epoch  11 : w =  1.9991778135299683  loss =  tensor(9.8190e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  21 : w =  1.9992018938064575  loss =  tensor(9.2468e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  31 : w =  1.9992256164550781  loss =  tensor(8.7073e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  41 : w =  1.9992483854293823  loss =  tensor(8.2023e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  51 : w =  1.9992706775665283  loss =  tensor(7.7257e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  61 : w =  1.9992921352386475  loss =  tensor(7.2754e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  71 : w =  1.9993129968643188  loss =  tensor(6.8528e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  81 : w =  1.9993332624435425  loss =  tensor(6.4540e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  91 : w =  1.9993529319763184  loss =  tensor(6.0791e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  101 : w =  1.9993720054626465  loss =  tensor(5.7259e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  111 : w =  1.9993906021118164  loss =  tensor(5.3930e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  121 : w =  1.9994086027145386  loss =  tensor(5.0787e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  131 : w =  1.9994261264801025  loss =  tensor(4.7829e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  141 : w =  1.9994429349899292  loss =  tensor(4.5057e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  151 : w =  1.9994595050811768  loss =  tensor(4.2424e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  161 : w =  1.999475359916687  loss =  tensor(3.9963e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  171 : w =  1.9994908571243286  loss =  tensor(3.7634e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  181 : w =  1.9995059967041016  loss =  tensor(3.5441e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  191 : w =  1.9995203018188477  loss =  tensor(3.3388e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  201 : w =  1.9995346069335938  loss =  tensor(3.1444e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  211 : w =  1.999548316001892  loss =  tensor(2.9613e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  221 : w =  1.9995615482330322  loss =  tensor(2.7902e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  231 : w =  1.9995746612548828  loss =  tensor(2.6277e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  241 : w =  1.9995871782302856  loss =  tensor(2.4741e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  251 : w =  1.9995992183685303  loss =  tensor(2.3311e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  261 : w =  1.9996111392974854  loss =  tensor(2.1957e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  271 : w =  1.9996227025985718  loss =  tensor(2.0677e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  281 : w =  1.9996337890625  loss =  tensor(1.9476e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  291 : w =  1.9996445178985596  loss =  tensor(1.8339e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  301 : w =  1.9996552467346191  loss =  tensor(1.7268e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  311 : w =  1.9996652603149414  loss =  tensor(1.6271e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  321 : w =  1.999674916267395  loss =  tensor(1.5321e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  331 : w =  1.999684453010559  loss =  tensor(1.4435e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  341 : w =  1.9996939897537231  loss =  tensor(1.3601e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  351 : w =  1.9997031688690186  loss =  tensor(1.2808e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  361 : w =  1.9997117519378662  loss =  tensor(1.2066e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  371 : w =  1.9997200965881348  loss =  tensor(1.1360e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  381 : w =  1.9997284412384033  loss =  tensor(1.0700e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  391 : w =  1.9997366666793823  loss =  tensor(1.0077e-07, grad_fn=<MseLossBackward>)\n",
      "epoch  401 : w =  1.9997442960739136  loss =  tensor(9.4860e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  411 : w =  1.9997518062591553  loss =  tensor(8.9435e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  421 : w =  1.9997589588165283  loss =  tensor(8.4213e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  431 : w =  1.9997661113739014  loss =  tensor(7.9363e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  441 : w =  1.9997732639312744  loss =  tensor(7.4743e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  451 : w =  1.9997799396514893  loss =  tensor(7.0385e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  461 : w =  1.9997862577438354  loss =  tensor(6.6271e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  471 : w =  1.9997925758361816  loss =  tensor(6.2427e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  481 : w =  1.9997985363006592  loss =  tensor(5.8831e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  491 : w =  1.9998044967651367  loss =  tensor(5.5382e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  501 : w =  1.9998104572296143  loss =  tensor(5.2207e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  511 : w =  1.9998160600662231  loss =  tensor(4.9128e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  521 : w =  1.999821424484253  loss =  tensor(4.6282e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  531 : w =  1.9998266696929932  loss =  tensor(4.3609e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  541 : w =  1.9998316764831543  loss =  tensor(4.1080e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  551 : w =  1.9998365640640259  loss =  tensor(3.8711e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  561 : w =  1.999841332435608  loss =  tensor(3.6444e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  571 : w =  1.99984610080719  loss =  tensor(3.4385e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  581 : w =  1.999850869178772  loss =  tensor(3.2381e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  591 : w =  1.9998552799224854  loss =  tensor(3.0444e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  601 : w =  1.9998594522476196  loss =  tensor(2.8674e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  611 : w =  1.999863624572754  loss =  tensor(2.7023e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  621 : w =  1.999867558479309  loss =  tensor(2.5441e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  631 : w =  1.9998714923858643  loss =  tensor(2.3980e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  641 : w =  1.9998750686645508  loss =  tensor(2.2611e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  651 : w =  1.9998787641525269  loss =  tensor(2.1280e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  661 : w =  1.9998823404312134  loss =  tensor(2.0046e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  671 : w =  1.9998859167099  loss =  tensor(1.8904e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  681 : w =  1.9998894929885864  loss =  tensor(1.7791e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  691 : w =  1.9998927116394043  loss =  tensor(1.6737e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  701 : w =  1.9998959302902222  loss =  tensor(1.5773e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  711 : w =  1.9998990297317505  loss =  tensor(1.4833e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  721 : w =  1.9999018907546997  loss =  tensor(1.3970e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  731 : w =  1.999904751777649  loss =  tensor(1.3177e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  741 : w =  1.9999074935913086  loss =  tensor(1.2403e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  751 : w =  1.9999101161956787  loss =  tensor(1.1679e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  761 : w =  1.9999127388000488  loss =  tensor(1.1014e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  771 : w =  1.9999152421951294  loss =  tensor(1.0373e-08, grad_fn=<MseLossBackward>)\n",
      "epoch  781 : w =  1.9999176263809204  loss =  tensor(9.7873e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  791 : w =  1.9999200105667114  loss =  tensor(9.2189e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  801 : w =  1.9999223947525024  loss =  tensor(8.6994e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  811 : w =  1.9999247789382935  loss =  tensor(8.1951e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  821 : w =  1.9999271631240845  loss =  tensor(7.7061e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  831 : w =  1.999929428100586  loss =  tensor(7.2675e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  841 : w =  1.9999315738677979  loss =  tensor(6.8408e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  851 : w =  1.9999334812164307  loss =  tensor(6.4271e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  861 : w =  1.9999353885650635  loss =  tensor(6.0583e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  871 : w =  1.9999372959136963  loss =  tensor(5.7004e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  881 : w =  1.999939203262329  loss =  tensor(5.3812e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  891 : w =  1.9999409914016724  loss =  tensor(5.0579e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  901 : w =  1.9999427795410156  loss =  tensor(4.7733e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  911 : w =  1.9999443292617798  loss =  tensor(4.4834e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  921 : w =  1.999945878982544  loss =  tensor(4.2249e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  931 : w =  1.9999475479125977  loss =  tensor(3.9892e-09, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  941 : w =  1.9999489784240723  loss =  tensor(3.7574e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  951 : w =  1.9999505281448364  loss =  tensor(3.5460e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  961 : w =  1.9999518394470215  loss =  tensor(3.3406e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  971 : w =  1.999953269958496  loss =  tensor(3.1543e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  981 : w =  1.9999545812606812  loss =  tensor(2.9682e-09, grad_fn=<MseLossBackward>)\n",
      "epoch  991 : w =  1.9999558925628662  loss =  tensor(2.8009e-09, grad_fn=<MseLossBackward>)\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "    #l = loss(y_predicted, Y)\n",
    "    \n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()  # unpack parameters\n",
    "        print('epoch ', epoch + 1, ': w = ', w[0][0].item(), ' loss = ', l)\n",
    "    \n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
