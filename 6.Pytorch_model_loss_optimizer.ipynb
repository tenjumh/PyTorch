{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) Define Dataset\n",
    "- 2) Define Model\n",
    "- 3) Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 4, #features: 1\n",
      "Prediction before training: f(5) = -1.245\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_sample, n_feature = X.shape\n",
    "#print(n_sample, n_feature)\n",
    "print(f'#samples: {n_sample}, #features: {n_feature}')\n",
    "\n",
    "input_size = n_feature\n",
    "output_size = n_feature\n",
    "\n",
    "#model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define diferent layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# defice loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_iters = 1000\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) epoch 정의\n",
    "- 2) 정의된 model에 일단 X를 넣어서 예측하는 변수 지정(y_predicted)\n",
    "- 3) 정의된 loss에 Y와 y_predicted 넣어서 변수 지정(I)\n",
    "- 4) forward만 했으니 loss value를 대상으로 backward()\n",
    "- 5) 그리고 optimizer.step()\n",
    "- 6) optimizer.zero_grad() 초기화???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1 : w =  1.9830141067504883  loss =  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
      "epoch  11 : w =  1.9835158586502075  loss =  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
      "epoch  21 : w =  1.9840028285980225  loss =  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
      "epoch  31 : w =  1.9844753742218018  loss =  tensor(0.0004, grad_fn=<MeanBackward0>)\n",
      "epoch  41 : w =  1.984933853149414  loss =  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
      "epoch  51 : w =  1.9853788614273071  loss =  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
      "epoch  61 : w =  1.9858108758926392  loss =  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
      "epoch  71 : w =  1.9862298965454102  loss =  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
      "epoch  81 : w =  1.986636757850647  loss =  tensor(0.0003, grad_fn=<MeanBackward0>)\n",
      "epoch  91 : w =  1.9870315790176392  loss =  tensor(0.0002, grad_fn=<MeanBackward0>)\n",
      "epoch  101 : w =  1.9874147176742554  loss =  tensor(0.0002, grad_fn=<MeanBackward0>)\n",
      "epoch  111 : w =  1.9877862930297852  loss =  tensor(0.0002, grad_fn=<MeanBackward0>)\n",
      "epoch  121 : w =  1.9881471395492554  loss =  tensor(0.0002, grad_fn=<MeanBackward0>)\n",
      "epoch  131 : w =  1.988497257232666  loss =  tensor(0.0002, grad_fn=<MeanBackward0>)\n",
      "epoch  141 : w =  1.9888370037078857  loss =  tensor(0.0002, grad_fn=<MeanBackward0>)\n",
      "epoch  151 : w =  1.9891668558120728  loss =  tensor(0.0002, grad_fn=<MeanBackward0>)\n",
      "epoch  161 : w =  1.989486813545227  loss =  tensor(0.0002, grad_fn=<MeanBackward0>)\n",
      "epoch  171 : w =  1.9897973537445068  loss =  tensor(0.0002, grad_fn=<MeanBackward0>)\n",
      "epoch  181 : w =  1.9900987148284912  loss =  tensor(0.0001, grad_fn=<MeanBackward0>)\n",
      "epoch  191 : w =  1.9903912544250488  loss =  tensor(0.0001, grad_fn=<MeanBackward0>)\n",
      "epoch  201 : w =  1.9906749725341797  loss =  tensor(0.0001, grad_fn=<MeanBackward0>)\n",
      "epoch  211 : w =  1.9909504652023315  loss =  tensor(0.0001, grad_fn=<MeanBackward0>)\n",
      "epoch  221 : w =  1.991217851638794  loss =  tensor(0.0001, grad_fn=<MeanBackward0>)\n",
      "epoch  231 : w =  1.9914772510528564  loss =  tensor(0.0001, grad_fn=<MeanBackward0>)\n",
      "epoch  241 : w =  1.9917290210723877  loss =  tensor(9.9358e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  251 : w =  1.9919732809066772  loss =  tensor(9.3575e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  261 : w =  1.9922103881835938  loss =  tensor(8.8128e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  271 : w =  1.9924404621124268  loss =  tensor(8.2998e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  281 : w =  1.9926637411117554  loss =  tensor(7.8168e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  291 : w =  1.9928804636001587  loss =  tensor(7.3618e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  301 : w =  1.9930907487869263  loss =  tensor(6.9334e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  311 : w =  1.9932948350906372  loss =  tensor(6.5297e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  321 : w =  1.9934929609298706  loss =  tensor(6.1498e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  331 : w =  1.9936851263046265  loss =  tensor(5.7917e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  341 : w =  1.9938716888427734  loss =  tensor(5.4546e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  351 : w =  1.994052767753601  loss =  tensor(5.1372e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  361 : w =  1.9942283630371094  loss =  tensor(4.8380e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  371 : w =  1.994398832321167  loss =  tensor(4.5565e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  381 : w =  1.9945642948150635  loss =  tensor(4.2914e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  391 : w =  1.9947248697280884  loss =  tensor(4.0416e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  401 : w =  1.9948806762695312  loss =  tensor(3.8062e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  411 : w =  1.9950318336486816  loss =  tensor(3.5848e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  421 : w =  1.9951785802841187  loss =  tensor(3.3761e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  431 : w =  1.9953210353851318  loss =  tensor(3.1797e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  441 : w =  1.9954593181610107  loss =  tensor(2.9945e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  451 : w =  1.9955934286117554  loss =  tensor(2.8202e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  461 : w =  1.9957236051559448  loss =  tensor(2.6562e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  471 : w =  1.995849847793579  loss =  tensor(2.5015e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  481 : w =  1.9959723949432373  loss =  tensor(2.3559e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  491 : w =  1.996091365814209  loss =  tensor(2.2188e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  501 : w =  1.9962068796157837  loss =  tensor(2.0897e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  511 : w =  1.9963189363479614  loss =  tensor(1.9680e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  521 : w =  1.9964275360107422  loss =  tensor(1.8535e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  531 : w =  1.9965331554412842  loss =  tensor(1.7457e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  541 : w =  1.9966355562210083  loss =  tensor(1.6440e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  551 : w =  1.9967349767684937  loss =  tensor(1.5483e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  561 : w =  1.9968314170837402  loss =  tensor(1.4582e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  571 : w =  1.9969249963760376  loss =  tensor(1.3733e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  581 : w =  1.9970158338546753  loss =  tensor(1.2934e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  591 : w =  1.9971039295196533  loss =  tensor(1.2182e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  601 : w =  1.9971895217895508  loss =  tensor(1.1473e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  611 : w =  1.9972726106643677  loss =  tensor(1.0804e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  621 : w =  1.9973530769348145  loss =  tensor(1.0176e-05, grad_fn=<MeanBackward0>)\n",
      "epoch  631 : w =  1.9974312782287598  loss =  tensor(9.5828e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  641 : w =  1.997507095336914  loss =  tensor(9.0253e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  651 : w =  1.997580885887146  loss =  tensor(8.4998e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  661 : w =  1.9976521730422974  loss =  tensor(8.0057e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  671 : w =  1.997721552848816  loss =  tensor(7.5395e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  681 : w =  1.997788906097412  loss =  tensor(7.1009e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  691 : w =  1.997854232788086  loss =  tensor(6.6875e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  701 : w =  1.997917652130127  loss =  tensor(6.2983e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  711 : w =  1.9979790449142456  loss =  tensor(5.9316e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  721 : w =  1.9980387687683105  loss =  tensor(5.5860e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  731 : w =  1.9980965852737427  loss =  tensor(5.2612e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  741 : w =  1.9981529712677002  loss =  tensor(4.9555e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  751 : w =  1.998207449913025  loss =  tensor(4.6666e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  761 : w =  1.9982603788375854  loss =  tensor(4.3954e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  771 : w =  1.9983118772506714  loss =  tensor(4.1394e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  781 : w =  1.9983617067337036  loss =  tensor(3.8983e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  791 : w =  1.9984101057052612  loss =  tensor(3.6712e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  801 : w =  1.9984570741653442  loss =  tensor(3.4579e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  811 : w =  1.9985026121139526  loss =  tensor(3.2566e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  821 : w =  1.998546838760376  loss =  tensor(3.0667e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  831 : w =  1.9985897541046143  loss =  tensor(2.8884e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  841 : w =  1.9986313581466675  loss =  tensor(2.7201e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  851 : w =  1.9986717700958252  loss =  tensor(2.5624e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  861 : w =  1.9987109899520874  loss =  tensor(2.4130e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  871 : w =  1.9987491369247437  loss =  tensor(2.2729e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  881 : w =  1.9987859725952148  loss =  tensor(2.1403e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  891 : w =  1.9988218545913696  loss =  tensor(2.0156e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  901 : w =  1.9988566637039185  loss =  tensor(1.8987e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  911 : w =  1.9988903999328613  loss =  tensor(1.7881e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  921 : w =  1.9989230632781982  loss =  tensor(1.6842e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  931 : w =  1.9989548921585083  loss =  tensor(1.5860e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  941 : w =  1.9989858865737915  loss =  tensor(1.4936e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  951 : w =  1.9990158081054688  loss =  tensor(1.4068e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  961 : w =  1.99904465675354  loss =  tensor(1.3251e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  971 : w =  1.9990730285644531  loss =  tensor(1.2479e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  981 : w =  1.9991004467010498  loss =  tensor(1.1754e-06, grad_fn=<MeanBackward0>)\n",
      "epoch  991 : w =  1.99912691116333  loss =  tensor(1.1069e-06, grad_fn=<MeanBackward0>)\n",
      "Prediction after training: f(5) = 9.998\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass with our model\n",
    "    y_predicted = model(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "    \n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters()  # unpack parameters\n",
    "        print('epoch ', epoch + 1, ': w = ', w[0][0].item(), ' loss = ', l)\n",
    "    \n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
